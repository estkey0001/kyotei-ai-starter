import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
from sklearn.utils.class_weight import compute_class_weight
import joblib
import warnings
warnings.filterwarnings('ignore')

print("ğŸš€ æˆ¸ç”°ç«¶è‰‡2024å¹´ã‚¦ãƒ«ãƒˆãƒ©é«˜ç²¾åº¦å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv('data/coconala_2024/toda_2024.csv')
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")

# é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
print("ğŸ”§ é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ä¸­...")

target_cols = ['finish_position_1', 'finish_position_2', 'finish_position_3', 
               'finish_position_4', 'finish_position_5', 'finish_position_6']

enhanced_features = []
y = []

for idx, row in df.iterrows():
    positions = [row[col] for col in target_cols if pd.notna(row[col])]
    if 1.0 in positions:
        winner = positions.index(1.0)
        y.append(winner)
        
        feature_row = []
        
        # åŸºæœ¬æ°—è±¡ãƒ‡ãƒ¼ã‚¿
        temp = float(row.get('temperature', 15)) if pd.notna(row.get('temperature')) else 15
        wind = float(row.get('wind_speed', 2)) if pd.notna(row.get('wind_speed')) else 2
        wave = float(row.get('wave_height', 0)) if pd.notna(row.get('wave_height')) else 0
        
        feature_row.extend([temp, wind, wave])
        
        # å„è‰‡ã®è©³ç´°ç‰¹å¾´é‡
        racer_features = []
        for i in range(1, 7):
            # åŸºæœ¬æˆç¸¾
            win_rate = float(row.get(f'win_rate_national_{i}', 5)) if pd.notna(row.get(f'win_rate_national_{i}')) else 5
            place_rate = float(row.get(f'place_rate_2_national_{i}', 30)) if pd.notna(row.get(f'place_rate_2_national_{i}')) else 30
            place_rate3 = float(row.get(f'place_rate_3_national_{i}', 50)) if pd.notna(row.get(f'place_rate_3_national_{i}')) else 50
            
            # é¸æ‰‹ãƒ‡ãƒ¼ã‚¿
            age = float(row.get(f'racer_age_{i}', 35)) if pd.notna(row.get(f'racer_age_{i}')) else 35
            weight = float(row.get(f'racer_weight_{i}', 52)) if pd.notna(row.get(f'racer_weight_{i}')) else 52
            
            # ãƒ¢ãƒ¼ã‚¿ãƒ¼ãƒ»ãƒœãƒ¼ãƒˆ
            motor_rate = float(row.get(f'motor_win_rate_{i}', 35)) if pd.notna(row.get(f'motor_win_rate_{i}')) else 35
            motor_place = float(row.get(f'motor_place_rate_3_{i}', 55)) if pd.notna(row.get(f'motor_place_rate_3_{i}')) else 55
            
            # ã‚¹ã‚¿ãƒ¼ãƒˆãƒ»å±•ç¤º
            start_timing = float(row.get(f'avg_start_timing_{i}', 0.15)) if pd.notna(row.get(f'avg_start_timing_{i}')) else 0.15
            exhibition_time = float(row.get(f'exhibition_time_{i}', 6.7)) if pd.notna(row.get(f'exhibition_time_{i}')) else 6.7
            
            # ã‚¯ãƒ©ã‚¹åŠ¹æœ
            racer_class = row.get(f'racer_class_{i}', 'B1')
            class_bonus = {'A1': 4, 'A2': 3, 'B1': 2, 'B2': 1}.get(racer_class, 2)
            
            # ç‰¹å¾´é‡çµ±åˆ
            racer_features.extend([
                win_rate, place_rate, place_rate3, age, weight,
                motor_rate, motor_place, start_timing, exhibition_time, class_bonus
            ])
        
        feature_row.extend(racer_features)
        
        # ç›¸å¯¾ç‰¹å¾´é‡ï¼ˆ1å·è‰‡åŸºæº–ï¼‰
        if len(racer_features) >= 60:  # 6è‰‡Ã—10ç‰¹å¾´é‡
            boat1_win = racer_features[0]  # 1å·è‰‡å‹ç‡
            for i in range(1, 6):  # 2-6å·è‰‡
                other_win = racer_features[i*10]  # å„è‰‡å‹ç‡
                feature_row.append(boat1_win - other_win)  # å‹ç‡å·®
        
        # ã‚ªãƒƒã‚ºæƒ…å ±ï¼ˆã‚ã‚Œã°ï¼‰
        odds = float(row.get('win_odds', 2.5)) if pd.notna(row.get('win_odds')) else 2.5
        feature_row.append(odds)
        
        enhanced_features.append(feature_row)

X = np.array(enhanced_features, dtype=float)
y = np.array(y)

print(f"å¼·åŒ–å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X)}ãƒ¬ãƒ¼ã‚¹, {X.shape[1]}ç‰¹å¾´é‡")
print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y)}")

# æ¬ æå€¤æœ€çµ‚å‡¦ç†
imputer = SimpleImputer(strategy='median')
X = imputer.fit_transform(X)

# ç‰¹å¾´é‡æ¨™æº–åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)

print("ğŸ¤– é«˜ç²¾åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’å®Ÿè¡Œä¸­...")

# ã‚¯ãƒ©ã‚¹é‡ã¿è¨ˆç®—ï¼ˆä¸å‡è¡¡å¯¾å¿œï¼‰
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))

# é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ç¾¤
models = {
    'RandomForest': RandomForestClassifier(
        n_estimators=300, max_depth=15, min_samples_split=5,
        class_weight='balanced', random_state=42, n_jobs=-1
    ),
    'ExtraTrees': ExtraTreesClassifier(
        n_estimators=300, max_depth=15, min_samples_split=5,
        class_weight='balanced', random_state=42, n_jobs=-1
    ),
    'LogisticRegression': LogisticRegression(
        class_weight='balanced', random_state=42, max_iter=1000
    )
}

model_scores = {}
trained_models = {}

for name, model in models.items():
    try:
        cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy')
        mean_score = cv_scores.mean()
        
        print(f"{name}: CVç²¾åº¦ = {mean_score:.4f} (Â±{cv_scores.std()*2:.4f})")
        
        model.fit(X_train, y_train)
        model_scores[name] = mean_score
        trained_models[name] = model
        
    except Exception as e:
        print(f"{name}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
print("ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬å®Ÿè¡Œä¸­...")
ensemble_preds = []
for model in trained_models.values():
    pred = model.predict_proba(X_test)
    ensemble_preds.append(pred)

# é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
ensemble_avg = np.mean(ensemble_preds, axis=0)
final_pred = np.argmax(ensemble_avg, axis=1)

# æœ€çµ‚ç²¾åº¦è©•ä¾¡
final_accuracy = accuracy_score(y_test, final_pred)

print(f"\nğŸ† æœ€çµ‚ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ç²¾åº¦: {final_accuracy:.4f} ({final_accuracy*100:.1f}%)")

# åŸºæº–ã¨ã®æ¯”è¼ƒ
base_accuracy = 0.823
if final_accuracy > base_accuracy:
    improvement = ((final_accuracy - base_accuracy) / base_accuracy * 100)
    print(f"ğŸ¯ ç²¾åº¦å‘ä¸Š: +{improvement:.1f}%")
else:
    decline = ((base_accuracy - final_accuracy) / base_accuracy * 100)
    print(f"âš ï¸ ç²¾åº¦: ãƒ™ãƒ¼ã‚¹æ¯”-{decline:.1f}% (æ”¹å–„è¦)")

print(f"\nğŸ“Š æœŸå¾…å€¤åˆ†æ:")
print(f"ãƒ©ãƒ³ãƒ€ãƒ äºˆæ¸¬ç²¾åº¦: {1/6:.3f} (16.7%)")
print(f"1å·è‰‡å›ºå®šäºˆæ¸¬: {np.sum(y_test==0)/len(y_test):.3f}")
print(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬: {final_accuracy:.3f}")

# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
print("\nğŸ“‹ è©³ç´°åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, final_pred, target_names=[f'{i+1}å·è‰‡' for i in range(6)]))

# ã‚¦ãƒ«ãƒˆãƒ©ãƒ¢ãƒ‡ãƒ«ä¿å­˜
ultra_model_path = 'toda_2024_ultra_high_accuracy_ensemble.pkl'
joblib.dump({
    'models': trained_models,
    'imputer': imputer,
    'scaler': scaler,
    'ensemble_weights': [1/len(trained_models)] * len(trained_models)
}, ultra_model_path)

print(f"ğŸ’¾ ã‚¦ãƒ«ãƒˆãƒ©ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {ultra_model_path}")
print("ğŸ‰ ã‚¦ãƒ«ãƒˆãƒ©é«˜ç²¾åº¦å­¦ç¿’å®Œäº†ï¼")
