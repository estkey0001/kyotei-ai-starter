# ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  v12.1 - XGBoostä¿®æ­£ç‰ˆ
# ãƒ•ã‚¡ã‚¤ãƒ«: kyotei_ai_xgboost_fixed_v12.py

import os
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
import json
from datetime import datetime, timedelta
import random
from typing import Dict, List, Tuple, Any
import logging

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore")

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class XGBoostKyoteiSystem:
    """
    ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  v12.1 - XGBoostä¿®æ­£ç‰ˆ
    XGBoost + RandomForest + GradientBoosting + NeuralNetwork ã®4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    
    ä¸»è¦æ©Ÿèƒ½:
    - 18æ¬¡å…ƒç‰¹å¾´é‡ã«ã‚ˆã‚‹é«˜ç²¾åº¦äºˆæ¸¬
    - 4ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
    - ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰¹å¾´é‡è¨ˆç®—
    - è²·ã„ç›®è‡ªå‹•ç”Ÿæˆ
    - å®Œå…¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    """

    def __init__(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.models = {}
        self.scaler = StandardScaler()
        self.feature_names = [
            "avg_st", "win_rate", "place_rate", "motor_rate", "boat_rate",
            "recent_performance", "course_advantage", "weather_factor",
            "wave_height", "wind_speed", "temperature", "humidity",
            "experience_years", "weight", "age", "morning_adjustment",
            "exhibition_time", "start_timing"
        ]

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿è¨­å®š (XGB, RF, GBM, NN)
        self.ensemble_weights = [0.2, 0.2, 0.25, 0.35]

        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.model_configs = {
            "xgboost": {
                "n_estimators": 200,
                "max_depth": 6,
                "learning_rate": 0.1,
                "subsample": 0.8,
                "colsample_bytree": 0.8,
                "random_state": 42,
                "objective": "reg:squarederror",
                "eval_metric": "rmse"
            },
            "random_forest": {
                "n_estimators": 150,
                "max_depth": 8,
                "min_samples_split": 5,
                "min_samples_leaf": 2,
                "random_state": 42
            },
            "gradient_boosting": {
                "n_estimators": 150,
                "max_depth": 6,
                "learning_rate": 0.1,
                "subsample": 0.8,
                "random_state": 42
            },
            "neural_network": {
                "hidden_layer_sizes": (100, 50, 25),
                "activation": "relu",
                "solver": "adam",
                "alpha": 0.001,
                "learning_rate": "adaptive",
                "max_iter": 500,
                "random_state": 42
            }
        }

        logger.info("XGBoostKyoteiSystem v12.1 åˆæœŸåŒ–å®Œäº†")

    def load_data(self, data_source: str = "sample") -> pd.DataFrame:
        """
        ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯å®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
        
        Args:
            data_source: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ ("sample" ã¾ãŸã¯ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹)
            
        Returns:
            pd.DataFrame: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿
        """
        try:
            if data_source == "sample":
                logger.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
                return self._generate_sample_data()
            else:
                if os.path.exists(data_source):
                    logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {data_source}")
                    return pd.read_csv(data_source)
                else:
                    logger.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_source}")
                    logger.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™")
                    return self._generate_sample_data()
                    
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            logger.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™")
            return self._generate_sample_data()

    def _generate_sample_data(self, n_samples: int = 1000) -> pd.DataFrame:
        """
        ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        
        Args:
            n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            pd.DataFrame: ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
        """
        np.random.seed(42)
        
        data = []
        for i in range(n_samples):
            # åŸºæœ¬ç‰¹å¾´é‡ç”Ÿæˆ
            avg_st = np.random.normal(0.17, 0.05)
            win_rate = np.random.uniform(0.05, 0.35)
            place_rate = np.random.uniform(0.3, 0.7)
            motor_rate = np.random.uniform(0.3, 0.8)
            boat_rate = np.random.uniform(0.3, 0.8)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
            recent_performance = np.random.uniform(0.2, 0.8)
            course_advantage = np.random.uniform(0.1, 0.9)
            
            # ç’°å¢ƒè¦å› 
            weather_factor = np.random.uniform(0.3, 1.0)
            wave_height = np.random.uniform(0, 5)
            wind_speed = np.random.uniform(0, 15)
            temperature = np.random.uniform(10, 35)
            humidity = np.random.uniform(30, 90)
            
            # é¸æ‰‹æƒ…å ±
            experience_years = np.random.randint(1, 30)
            weight = np.random.uniform(45, 65)
            age = np.random.randint(20, 60)
            
            # ãƒ¬ãƒ¼ã‚¹å½“æ—¥è¦å› 
            morning_adjustment = np.random.uniform(-0.1, 0.1)
            exhibition_time = np.random.uniform(6.8, 7.5)
            start_timing = np.random.uniform(-0.2, 0.2)
            
            # ç€é †ï¼ˆç›®çš„å¤‰æ•°ï¼‰- ç‰¹å¾´é‡ã«åŸºã¥ã„ã¦ç”Ÿæˆ
            rank_score = (
                win_rate * 0.3 + 
                place_rate * 0.2 + 
                motor_rate * 0.15 + 
                boat_rate * 0.15 + 
                recent_performance * 0.1 + 
                course_advantage * 0.1 +
                np.random.normal(0, 0.1)
            )
            
            # ç€é †ã«å¤‰æ›ï¼ˆ1-6ä½ï¼‰
            rank = max(1, min(6, int(7 - rank_score * 6)))
            
            data.append([
                avg_st, win_rate, place_rate, motor_rate, boat_rate,
                recent_performance, course_advantage, weather_factor,
                wave_height, wind_speed, temperature, humidity,
                experience_years, weight, age, morning_adjustment,
                exhibition_time, start_timing, rank
            ])
        
        columns = self.feature_names + ["rank"]
        df = pd.DataFrame(data, columns=columns)
        
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(df)}ä»¶")
        return df

    def get_race_data(self, race_id: str = "sample_race") -> Dict[str, Any]:
        """
        ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
        
        Args:
            race_id: ãƒ¬ãƒ¼ã‚¹ID
            
        Returns:
            Dict: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
        """
        try:
            race_data = {
                "race_id": race_id,
                "date": datetime.now().strftime("%Y-%m-%d"),
                "venue": "ã‚µãƒ³ãƒ—ãƒ«ç«¶è‰‡å ´",
                "race_number": np.random.randint(1, 12),
                "weather": np.random.choice(["æ™´", "æ›‡", "é›¨"]),
                "wind_direction": np.random.choice(["ç„¡é¢¨", "å‘é¢¨", "è¿½é¢¨", "æ¨ªé¢¨"]),
                "wind_speed": np.random.uniform(0, 10),
                "wave_height": np.random.uniform(0, 3),
                "temperature": np.random.uniform(15, 30),
                "humidity": np.random.uniform(40, 80),
                "boats": []
            }
            
            # 6è‰‡ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            for boat_num in range(1, 7):
                boat_data = {
                    "boat_number": boat_num,
                    "racer_name": f"é¸æ‰‹{boat_num}",
                    "avg_st": np.random.normal(0.17, 0.05),
                    "win_rate": np.random.uniform(0.05, 0.35),
                    "place_rate": np.random.uniform(0.3, 0.7),
                    "motor_number": np.random.randint(1, 60),
                    "motor_rate": np.random.uniform(0.3, 0.8),
                    "boat_number_actual": np.random.randint(1, 60),
                    "boat_rate": np.random.uniform(0.3, 0.8),
                    "weight": np.random.uniform(45, 65),
                    "age": np.random.randint(20, 60),
                    "experience_years": np.random.randint(1, 30)
                }
                race_data["boats"].append(boat_data)
            
            logger.info(f"ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {race_id}")
            return race_data
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def calculate_professional_features(self, race_data: Dict[str, Any]) -> np.ndarray:
        """
        ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ç‰¹å¾´é‡è¨ˆç®—
        
        Args:
            race_data: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            np.ndarray: ç‰¹å¾´é‡è¡Œåˆ— (6è‰‡ Ã— 18ç‰¹å¾´é‡)
        """
        try:
            features = []
            
            for boat in race_data["boats"]:
                # åŸºæœ¬ç‰¹å¾´é‡
                avg_st = boat.get("avg_st", 0.17)
                win_rate = boat.get("win_rate", 0.15)
                place_rate = boat.get("place_rate", 0.45)
                motor_rate = boat.get("motor_rate", 0.5)
                boat_rate = boat.get("boat_rate", 0.5)
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—
                recent_performance = (win_rate + place_rate) / 2
                course_advantage = 0.8 if boat["boat_number"] == 1 else 0.6 if boat["boat_number"] <= 3 else 0.4
                
                # ç’°å¢ƒè¦å› 
                weather_factor = self._calculate_weather_factor(race_data)
                wave_height = race_data.get("wave_height", 1.0)
                wind_speed = race_data.get("wind_speed", 3.0)
                temperature = race_data.get("temperature", 20.0)
                humidity = race_data.get("humidity", 60.0)
                
                # é¸æ‰‹æƒ…å ±
                experience_years = boat.get("experience_years", 10)
                weight = boat.get("weight", 52.0)
                age = boat.get("age", 35)
                
                # ãƒ¬ãƒ¼ã‚¹å½“æ—¥è¦å› 
                morning_adjustment = np.random.uniform(-0.05, 0.05)
                exhibition_time = np.random.uniform(6.9, 7.3)
                start_timing = avg_st + np.random.uniform(-0.1, 0.1)
                
                boat_features = [
                    avg_st, win_rate, place_rate, motor_rate, boat_rate,
                    recent_performance, course_advantage, weather_factor,
                    wave_height, wind_speed, temperature, humidity,
                    experience_years, weight, age, morning_adjustment,
                    exhibition_time, start_timing
                ]
                
                features.append(boat_features)
            
            features_array = np.array(features)
            logger.info(f"ç‰¹å¾´é‡è¨ˆç®—å®Œäº†: {features_array.shape}")
            return features_array
            
        except Exception as e:
            logger.error(f"ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return np.random.rand(6, 18)

    def _calculate_weather_factor(self, race_data: Dict[str, Any]) -> float:
        """å¤©å€™è¦å› è¨ˆç®—"""
        weather = race_data.get("weather", "æ™´")
        wind_speed = race_data.get("wind_speed", 3.0)
        
        weather_base = {"æ™´": 1.0, "æ›‡": 0.9, "é›¨": 0.7}.get(weather, 0.8)
        wind_factor = max(0.5, 1.0 - wind_speed * 0.05)
        
        return weather_base * wind_factor

    def train_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """
        4ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
        
        Args:
            X: ç‰¹å¾´é‡
            y: ç›®çš„å¤‰æ•°
            
        Returns:
            Dict: è¨“ç·´çµæœ
        """
        try:
            logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
            
            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )
            
            # ç‰¹å¾´é‡æ­£è¦åŒ–
            X_train_scaled = self.scaler.fit_transform(X_train)
            X_test_scaled = self.scaler.transform(X_test)
            
            results = {}
            
            # 1. XGBoost
            logger.info("XGBoostè¨“ç·´ä¸­...")
            self.models["xgboost"] = xgb.XGBRegressor(**self.model_configs["xgboost"])
            self.models["xgboost"].fit(X_train, y_train)
            xgb_pred = self.models["xgboost"].predict(X_test)
            results["xgboost"] = {
                "mse": mean_squared_error(y_test, xgb_pred),
                "mae": mean_absolute_error(y_test, xgb_pred)
            }
            
            # 2. Random Forest
            logger.info("Random Forestè¨“ç·´ä¸­...")
            self.models["random_forest"] = RandomForestRegressor(**self.model_configs["random_forest"])
            self.models["random_forest"].fit(X_train, y_train)
            rf_pred = self.models["random_forest"].predict(X_test)
            results["random_forest"] = {
                "mse": mean_squared_error(y_test, rf_pred),
                "mae": mean_absolute_error(y_test, rf_pred)
            }
            
            # 3. Gradient Boosting
            logger.info("Gradient Boostingè¨“ç·´ä¸­...")
            self.models["gradient_boosting"] = GradientBoostingRegressor(**self.model_configs["gradient_boosting"])
            self.models["gradient_boosting"].fit(X_train, y_train)
            gb_pred = self.models["gradient_boosting"].predict(X_test)
            results["gradient_boosting"] = {
                "mse": mean_squared_error(y_test, gb_pred),
                "mae": mean_absolute_error(y_test, gb_pred)
            }
            
            # 4. Neural Network
            logger.info("Neural Networkè¨“ç·´ä¸­...")
            self.models["neural_network"] = MLPRegressor(**self.model_configs["neural_network"])
            self.models["neural_network"].fit(X_train_scaled, y_train)
            nn_pred = self.models["neural_network"].predict(X_test_scaled)
            results["neural_network"] = {
                "mse": mean_squared_error(y_test, nn_pred),
                "mae": mean_absolute_error(y_test, nn_pred)
            }
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            ensemble_pred = (
                xgb_pred * self.ensemble_weights[0] +
                rf_pred * self.ensemble_weights[1] +
                gb_pred * self.ensemble_weights[2] +
                nn_pred * self.ensemble_weights[3]
            )
            
            results["ensemble"] = {
                "mse": mean_squared_error(y_test, ensemble_pred),
                "mae": mean_absolute_error(y_test, ensemble_pred)
            }
            
            logger.info("ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
            return results
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def analyze_race_xgboost(self, race_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        XGBoostã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã‚ˆã‚‹ãƒ¬ãƒ¼ã‚¹åˆ†æ
        
        Args:
            race_data: ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            Dict: åˆ†æçµæœ
        """
        try:
            logger.info("ãƒ¬ãƒ¼ã‚¹åˆ†æé–‹å§‹...")
            
            # ç‰¹å¾´é‡è¨ˆç®—
            features = self.calculate_professional_features(race_data)
            
            if not self.models:
                logger.warning("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return self._generate_dummy_analysis(race_data)
            
            predictions = {}
            
            # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            for model_name, model in self.models.items():
                if model_name == "neural_network":
                    features_scaled = self.scaler.transform(features)
                    pred = model.predict(features_scaled)
                else:
                    pred = model.predict(features)
                
                predictions[model_name] = pred
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
            ensemble_pred = (
                predictions["xgboost"] * self.ensemble_weights[0] +
                predictions["random_forest"] * self.ensemble_weights[1] +
                predictions["gradient_boosting"] * self.ensemble_weights[2] +
                predictions["neural_network"] * self.ensemble_weights[3]
            )
            
            # ç€é †äºˆæ¸¬ï¼ˆäºˆæ¸¬å€¤ã®é€†é †ï¼‰
            predicted_ranks = np.argsort(ensemble_pred) + 1
            
            # çµæœæ•´ç†
            analysis_result = {
                "race_id": race_data.get("race_id", "unknown"),
                "predictions": {
                    "ensemble": ensemble_pred.tolist(),
                    "individual_models": {k: v.tolist() for k, v in predictions.items()},
                    "predicted_ranks": predicted_ranks.tolist()
                },
                "boat_analysis": []
            }
            
            # å„è‰‡ã®åˆ†æ
            for i, boat in enumerate(race_data["boats"]):
                boat_analysis = {
                    "boat_number": boat["boat_number"],
                    "racer_name": boat.get("racer_name", f"é¸æ‰‹{i+1}"),
                    "predicted_rank": int(predicted_ranks[i]),
                    "confidence_score": float(ensemble_pred[i]),
                    "win_probability": max(0, min(1, (6 - ensemble_pred[i]) / 5)),
                    "features": features[i].tolist()
                }
                analysis_result["boat_analysis"].append(boat_analysis)
            
            # ç€é †ã§ã‚½ãƒ¼ãƒˆ
            analysis_result["boat_analysis"].sort(key=lambda x: x["predicted_rank"])
            
            logger.info("ãƒ¬ãƒ¼ã‚¹åˆ†æå®Œäº†")
            return analysis_result
            
        except Exception as e:
            logger.error(f"ãƒ¬ãƒ¼ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_dummy_analysis(race_data)

    def _generate_dummy_analysis(self, race_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ€ãƒŸãƒ¼åˆ†æçµæœç”Ÿæˆ"""
        dummy_result = {
            "race_id": race_data.get("race_id", "unknown"),
            "predictions": {
                "ensemble": [3.5, 2.1, 4.2, 1.8, 5.1, 3.9],
                "predicted_ranks": [4, 1, 5, 2, 6, 3]
            },
            "boat_analysis": []
        }
        
        for i, boat in enumerate(race_data["boats"]):
            boat_analysis = {
                "boat_number": boat["boat_number"],
                "racer_name": boat.get("racer_name", f"é¸æ‰‹{i+1}"),
                "predicted_rank": i + 1,
                "confidence_score": np.random.uniform(0.3, 0.8),
                "win_probability": np.random.uniform(0.1, 0.4)
            }
            dummy_result["boat_analysis"].append(boat_analysis)
        
        return dummy_result

    def generate_xgboost_formations(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        XGBooståˆ†æçµæœã‹ã‚‰è²·ã„ç›®ç”Ÿæˆ
        
        Args:
            analysis_result: åˆ†æçµæœ
            
        Returns:
            Dict: è²·ã„ç›®æƒ…å ±
        """
        try:
            logger.info("è²·ã„ç›®ç”Ÿæˆé–‹å§‹...")
            
            boat_analysis = analysis_result.get("boat_analysis", [])
            if not boat_analysis:
                return {"error": "åˆ†æçµæœãŒä¸æ­£ã§ã™"}
            
            # ä¿¡é ¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_boats = sorted(boat_analysis, key=lambda x: x["predicted_rank"])
            
            formations = {
                "race_id": analysis_result.get("race_id", "unknown"),
                "timestamp": datetime.now().isoformat(),
                "recommendations": []
            }
            
            # 3é€£å˜æ¨å¥¨
            top3 = sorted_boats[:3]
            trifecta = {
                "bet_type": "3é€£å˜",
                "combination": [boat["boat_number"] for boat in top3],
                "confidence": np.mean([boat["confidence_score"] for boat in top3]),
                "expected_odds": self._estimate_odds(top3, "3é€£å˜"),
                "recommended_amount": 1000
            }
            formations["recommendations"].append(trifecta)
            
            # 3é€£è¤‡æ¨å¥¨
            trifecta_box = {
                "bet_type": "3é€£è¤‡",
                "combination": sorted([boat["boat_number"] for boat in top3]),
                "confidence": np.mean([boat["confidence_score"] for boat in top3]) * 0.9,
                "expected_odds": self._estimate_odds(top3, "3é€£è¤‡"),
                "recommended_amount": 800
            }
            formations["recommendations"].append(trifecta_box)
            
            # 2é€£å˜æ¨å¥¨
            top2 = sorted_boats[:2]
            exacta = {
                "bet_type": "2é€£å˜",
                "combination": [boat["boat_number"] for boat in top2],
                "confidence": np.mean([boat["confidence_score"] for boat in top2]),
                "expected_odds": self._estimate_odds(top2, "2é€£å˜"),
                "recommended_amount": 1500
            }
            formations["recommendations"].append(exacta)
            
            # å˜å‹æ¨å¥¨
            win_bet = {
                "bet_type": "å˜å‹",
                "combination": [sorted_boats[0]["boat_number"]],
                "confidence": sorted_boats[0]["confidence_score"],
                "expected_odds": self._estimate_odds([sorted_boats[0]], "å˜å‹"),
                "recommended_amount": 2000
            }
            formations["recommendations"].append(win_bet)
            
            # ç·æŠ•è³‡é¡ã¨æœŸå¾…åç›Š
            total_investment = sum(rec["recommended_amount"] for rec in formations["recommendations"])
            expected_return = sum(
                rec["recommended_amount"] * rec["expected_odds"] * rec["confidence"]
                for rec in formations["recommendations"]
            )
            
            formations["summary"] = {
                "total_investment": total_investment,
                "expected_return": expected_return,
                "roi_estimate": (expected_return - total_investment) / total_investment * 100
            }
            
            logger.info("è²·ã„ç›®ç”Ÿæˆå®Œäº†")
            return formations
            
        except Exception as e:
            logger.error(f"è²·ã„ç›®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"è²·ã„ç›®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"}

    def _estimate_odds(self, boats: List[Dict], bet_type: str) -> float:
        """ã‚ªãƒƒã‚ºæ¨å®š"""
        base_odds = {
            "å˜å‹": 3.5,
            "2é€£å˜": 12.0,
            "3é€£å˜": 45.0,
            "3é€£è¤‡": 15.0
        }
        
        confidence_avg = np.mean([boat["confidence_score"] for boat in boats])
        adjustment = 1.0 / max(0.1, confidence_avg)
        
        return base_odds.get(bet_type, 10.0) * adjustment

    def save_analysis_results(self, analysis_result: Dict[str, Any], formations: Dict[str, Any]) -> str:
        """åˆ†æçµæœä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"kyotei_analysis_{timestamp}.json"
            
            save_data = {
                "system_version": "v12.1",
                "timestamp": datetime.now().isoformat(),
                "analysis_result": analysis_result,
                "formations": formations,
                "model_info": {
                    "ensemble_weights": self.ensemble_weights,
                    "feature_count": len(self.feature_names),
                    "models_used": list(self.models.keys()) if self.models else []
                }
            }
            
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"åˆ†æçµæœä¿å­˜å®Œäº†: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"åˆ†æçµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def run_complete_analysis(self, race_id: str = "demo_race") -> Dict[str, Any]:
        """å®Œå…¨åˆ†æå®Ÿè¡Œ"""
        try:
            logger.info(f"å®Œå…¨åˆ†æé–‹å§‹: {race_id}")
            
            # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            training_data = self.load_data("sample")
            
            # 2. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            X = training_data[self.feature_names].values
            y = training_data["rank"].values
            training_results = self.train_models(X, y)
            
            # 3. ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾—
            race_data = self.get_race_data(race_id)
            
            # 4. ãƒ¬ãƒ¼ã‚¹åˆ†æ
            analysis_result = self.analyze_race_xgboost(race_data)
            
            # 5. è²·ã„ç›®ç”Ÿæˆ
            formations = self.generate_xgboost_formations(analysis_result)
            
            # 6. çµæœä¿å­˜
            saved_file = self.save_analysis_results(analysis_result, formations)
            
            # 7. çµæœçµ±åˆ
            complete_result = {
                "status": "success",
                "race_id": race_id,
                "training_results": training_results,
                "analysis_result": analysis_result,
                "formations": formations,
                "saved_file": saved_file,
                "execution_time": datetime.now().isoformat()
            }
            
            logger.info("å®Œå…¨åˆ†æå®Œäº†")
            return complete_result
            
        except Exception as e:
            logger.error(f"å®Œå…¨åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "status": "error",
                "error_message": str(e),
                "race_id": race_id
            }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  v12.1 - XGBoostä¿®æ­£ç‰ˆ")
    print("=" * 60)
    
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        system = XGBoostKyoteiSystem()
        print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        
        # å®Œå…¨åˆ†æå®Ÿè¡Œ
        result = system.run_complete_analysis("test_race_001")
        
        if result["status"] == "success":
            print("\nğŸ¯ åˆ†æçµæœã‚µãƒãƒªãƒ¼:")
            print(f"ãƒ¬ãƒ¼ã‚¹ID: {result['race_id']}")
            
            # è¨“ç·´çµæœè¡¨ç¤º
            if result["training_results"]:
                print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½:")
                for model_name, metrics in result["training_results"].items():
                    print(f"  {model_name}: MSE={metrics['mse']:.4f}, MAE={metrics['mae']:.4f}")
            
            # äºˆæƒ³çµæœè¡¨ç¤º
            analysis = result["analysis_result"]
            if "boat_analysis" in analysis:
                print("\nğŸ ç€é †äºˆæƒ³:")
                for boat in analysis["boat_analysis"][:3]:
                    print(f"  {boat['predicted_rank']}ä½: {boat['boat_number']}å·è‰‡ "
                          f"({boat['racer_name']}) - ä¿¡é ¼åº¦: {boat['confidence_score']:.3f}")
            
            # è²·ã„ç›®æ¨å¥¨è¡¨ç¤º
            formations = result["formations"]
            if "recommendations" in formations:
                print("\nğŸ’° æ¨å¥¨è²·ã„ç›®:")
                for rec in formations["recommendations"]:
                    print(f"  {rec['bet_type']}: {rec['combination']} "
                          f"- ä¿¡é ¼åº¦: {rec['confidence']:.3f}, æœŸå¾…ã‚ªãƒƒã‚º: {rec['expected_odds']:.1f}å€")
            
            # æŠ•è³‡ã‚µãƒãƒªãƒ¼
            if "summary" in formations:
                summary = formations["summary"]
                print("\nğŸ“ˆ æŠ•è³‡ã‚µãƒãƒªãƒ¼:")
                print(f"  ç·æŠ•è³‡é¡: {summary['total_investment']:,}å††")
                print(f"  æœŸå¾…åç›Š: {summary['expected_return']:,.0f}å††")
                print(f"  ROIäºˆæƒ³: {summary['roi_estimate']:.1f}%")
            
            print(f"\nğŸ’¾ çµæœä¿å­˜: {result['saved_file']}")
            print("\nâœ… åˆ†æå®Œäº†ï¼")
            
        else:
            print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {result['error_message']}")
            
    except Exception as e:
        print(f"âŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
