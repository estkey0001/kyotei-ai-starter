import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
import joblib
import warnings
warnings.filterwarnings('ignore')

print("ğŸš€ æˆ¸ç”°ç«¶è‰‡2024å¹´å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹ï¼ˆä¿®æ­£ç‰ˆï¼‰")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv('data/coconala_2024/toda_2024.csv')
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
print(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['race_date'].min()} - {df['race_date'].max()}")

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
print("ğŸ”§ å‰å‡¦ç†å®Ÿè¡Œä¸­...")
target_cols = ['finish_position_1', 'finish_position_2', 'finish_position_3', 
               'finish_position_4', 'finish_position_5', 'finish_position_6']

y = []
features = []

for idx, row in df.iterrows():
    positions = [row[col] for col in target_cols if pd.notna(row[col])]
    if 1.0 in positions:
        winner = positions.index(1.0)
        y.append(winner)
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆæ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹ï¼‰
        feature_row = []
        
        # æ°—è±¡ãƒ‡ãƒ¼ã‚¿
        feature_row.extend([
            float(row.get('temperature', 0)) if pd.notna(row.get('temperature', 0)) else 0,
            float(row.get('wind_speed', 0)) if pd.notna(row.get('wind_speed', 0)) else 0,
            float(row.get('wave_height', 0)) if pd.notna(row.get('wave_height', 0)) else 0
        ])
        
        # å„è‰‡ã®ç‰¹å¾´é‡ï¼ˆæ¬ æå€¤å‡¦ç†ï¼‰
        for i in range(1, 7):
            win_rate = row.get(f'win_rate_national_{i}', 0)
            place_rate = row.get(f'place_rate_2_national_{i}', 0)
            age = row.get(f'racer_age_{i}', 0)
            motor_rate = row.get(f'motor_win_rate_{i}', 0)
            start_timing = row.get(f'avg_start_timing_{i}', 0)
            
            feature_row.extend([
                float(win_rate) if pd.notna(win_rate) else 0,
                float(place_rate) if pd.notna(place_rate) else 0,
                float(age) if pd.notna(age) else 0,
                float(motor_rate) if pd.notna(motor_rate) else 0,
                float(start_timing) if pd.notna(start_timing) else 0
            ])
        
        features.append(feature_row)

X = np.array(features, dtype=float)
y = np.array(y)

print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X)}ãƒ¬ãƒ¼ã‚¹, {X.shape[1]}ç‰¹å¾´é‡")
print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y)}")

# NaNå€¤æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒ»å‡¦ç†
print("ğŸ” æ¬ æå€¤å‡¦ç†ä¸­...")
imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)
print(f"æ¬ æå€¤å‡¦ç†å¾Œ: NaNæ•° = {np.isnan(X).sum()}")

# å­¦ç¿’å®Ÿè¡Œ
print("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ¬ æå€¤å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
models = {
    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42, max_depth=10),
    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42)
}

best_model = None
best_score = 0

for name, model in models.items():
    try:
        # äº¤å·®æ¤œè¨¼
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
        mean_score = cv_scores.mean()
        
        print(f"{name}: CVç²¾åº¦ = {mean_score:.4f} (Â±{cv_scores.std()*2:.4f})")
        
        if mean_score > best_score:
            best_score = mean_score
            best_model = model
    except Exception as e:
        print(f"{name}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’
print(f"ğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠ: {best_model.__class__.__name__}")
best_model.fit(X_train, y_train)

# ãƒ†ã‚¹ãƒˆç²¾åº¦è©•ä¾¡
test_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_pred)

print(f"âœ… ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")

# åŸºæº–ç²¾åº¦ã¨ã®æ¯”è¼ƒ
base_accuracy = 0.823
if test_accuracy > base_accuracy:
    improvement = ((test_accuracy - base_accuracy) / base_accuracy * 100)
    print(f"ğŸ¯ ç²¾åº¦å‘ä¸Š: +{improvement:.1f}%")
else:
    decline = ((base_accuracy - test_accuracy) / base_accuracy * 100)
    print(f"âš ï¸ ç²¾åº¦ä½ä¸‹: -{decline:.1f}%")

# è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
print("\nğŸ“‹ è©³ç´°åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
print(classification_report(y_test, test_pred, target_names=[f'{i+1}å·è‰‡' for i in range(6)]))

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
model_path = 'toda_2024_high_accuracy_model_fixed.pkl'
joblib.dump({
    'model': best_model,
    'imputer': imputer,
    'feature_names': ['temperature', 'wind_speed', 'wave_height'] + 
                    [f'{feat}_{i}' for i in range(1, 7) 
                     for feat in ['win_rate', 'place_rate', 'age', 'motor_rate', 'start_timing']]
}, model_path)
print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
