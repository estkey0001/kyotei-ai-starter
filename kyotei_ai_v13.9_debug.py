import streamlit as st
import pandas as pd
import numpy as np
import sqlite3
from pathlib import Path
import os
import glob
import chardet
import logging
from datetime import datetime, timedelta
import re
import warnings
warnings.filterwarnings('ignore')

# ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KyoteiDataManager:
    """ç«¶è‰‡ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¯ãƒ©ã‚¹ - ãƒ‘ã‚¹æ¤œç´¢ã¨ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½å¼·åŒ–ç‰ˆ"""

    def __init__(self):
        self.debug_info = {
            'searched_paths': [],
            'found_files': {},
            'errors': [],
            'encoding_results': {}
        }
        self.csv_data = {}
        self.db_connection = None
        self.available_races = []

        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®å€™è£œï¼ˆå„ªå…ˆé †ä½é †ï¼‰
        self.candidate_paths = [
            "~/kyotei-ai-starter/data/coconala_2024/",
            "~/kyotei-ai-starter/",  
            "./data/coconala_2024/",
            "./kyotei_data/",
            "./data/",
            "./"
        ]

        # DBãƒ•ã‚¡ã‚¤ãƒ«ã®å€™è£œ
        self.db_candidates = [
            "~/kyotei-ai-starter/kyotei_racer_master.db",
            "./kyotei_racer_master.db",
            "./data/kyotei_racer_master.db"
        ]

        self._initialize_data_paths()

    def _initialize_data_paths(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã®åˆæœŸåŒ–ã¨æ¤œç´¢"""
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...")

        # CSV ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œç´¢
        self.csv_data_path = self._search_csv_directory()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢
        self.db_path = self._search_database_file()

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        self._display_debug_info()

    def _search_csv_directory(self):
        """CSV ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢"""
        logger.info("CSV ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢ä¸­...")

        for candidate in self.candidate_paths:
            expanded_path = Path(candidate).expanduser().resolve()
            self.debug_info['searched_paths'].append(str(expanded_path))

            if expanded_path.exists() and expanded_path.is_dir():
                # CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                csv_files = list(expanded_path.glob("*.csv"))
                if csv_files:
                    self.debug_info['found_files'][str(expanded_path)] = [f.name for f in csv_files]
                    logger.info(f"CSV ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç™ºè¦‹: {expanded_path}")
                    logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")
                    return expanded_path

        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        current_path = Path(".").resolve()
        logger.warning(f"CSV ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨: {current_path}")
        return current_path

    def _search_database_file(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")

        for candidate in self.db_candidates:
            expanded_path = Path(candidate).expanduser().resolve()

            if expanded_path.exists() and expanded_path.is_file():
                logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™ºè¦‹: {expanded_path}")
                return expanded_path

        logger.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    def _detect_encoding(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º"""
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # æœ€åˆã®10KBã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º
                result = chardet.detect(raw_data)
                encoding = result['encoding']
                confidence = result['confidence']

                self.debug_info['encoding_results'][str(file_path)] = {
                    'encoding': encoding,
                    'confidence': confidence
                }

                logger.info(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡º: {file_path.name} -> {encoding} (ä¿¡é ¼åº¦: {confidence:.2f})")
                return encoding

        except Exception as e:
            logger.error(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
            return 'utf-8'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def _normalize_column_names(self, df):
        """åˆ—åã®æ­£è¦åŒ–"""
        original_columns = df.columns.tolist()

        # åˆ—åã®æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«
        normalized_columns = []
        for col in df.columns:
            # å‰å¾Œã®ç©ºç™½å‰Šé™¤
            normalized = col.strip()
            # å…¨è§’ãƒ»åŠè§’çµ±ä¸€
            normalized = normalized.replace('ã€€', ' ')
            # ç‰¹æ®Šæ–‡å­—ã®çµ±ä¸€
            normalized = re.sub(r'[ï¼ˆ(]', '(', normalized)
            normalized = re.sub(r'[ï¼‰)]', ')', normalized)
            normalized_columns.append(normalized)

        df.columns = normalized_columns

        if original_columns != normalized_columns:
            logger.info("åˆ—åã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸ")
            for orig, norm in zip(original_columns, normalized_columns):
                if orig != norm:
                    logger.info(f"  {orig} -> {norm}")

        return df

    def load_csv_data(self):
        """CSV ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info("CSV ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")

        if not self.csv_data_path.exists():
            error_msg = f"ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.csv_data_path}"
            self.debug_info['errors'].append(error_msg)
            logger.error(error_msg)
            return False

        csv_files = list(self.csv_data_path.glob("*.csv"))

        if not csv_files:
            error_msg = f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.csv_data_path}"
            self.debug_info['errors'].append(error_msg)
            logger.error(error_msg)
            return False

        logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(csv_files)}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        successful_loads = 0
        for csv_file in csv_files:
            try:
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ¤œå‡º
                encoding = self._detect_encoding(csv_file)

                # CSVèª­ã¿è¾¼ã¿
                df = pd.read_csv(csv_file, encoding=encoding)

                # åˆ—åæ­£è¦åŒ–
                df = self._normalize_column_names(df)

                # æ—¥ä»˜åˆ—ã®è‡ªå‹•å¤‰æ›
                df = self._auto_convert_dates(df)

                # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’ã‚­ãƒ¼ã¨ã—ã¦ä¿å­˜
                key = csv_file.stem
                self.csv_data[key] = df

                successful_loads += 1
                logger.info(f"èª­ã¿è¾¼ã¿æˆåŠŸ: {csv_file.name} (è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)})")

            except Exception as e:
                error_msg = f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {csv_file.name} - {e}"
                self.debug_info['errors'].append(error_msg)
                logger.error(error_msg)

        logger.info(f"CSVèª­ã¿è¾¼ã¿å®Œäº†: {successful_loads}/{len(csv_files)} ãƒ•ã‚¡ã‚¤ãƒ«æˆåŠŸ")
        return successful_loads > 0

    def _auto_convert_dates(self, df):
        """æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®è‡ªå‹•å¤‰æ›"""
        date_columns = []

        for col in df.columns:
            if any(keyword in col.lower() for keyword in ['date', 'æ—¥ä»˜', 'å¹´æœˆæ—¥', 'day']):
                date_columns.append(col)

        for col in date_columns:
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                logger.info(f"æ—¥ä»˜å¤‰æ›: {col}")
            except Exception as e:
                logger.warning(f"æ—¥ä»˜å¤‰æ›å¤±æ•—: {col} - {e}")

        return df

    def connect_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š"""
        if self.db_path is None:
            logger.warning("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€DBæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
            return False

        try:
            self.db_connection = sqlite3.connect(str(self.db_path))
            logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ: {self.db_path}")
            return True
        except Exception as e:
            error_msg = f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}"
            self.debug_info['errors'].append(error_msg)
            logger.error(error_msg)
            return False

    def get_available_races(self):
        """åˆ©ç”¨å¯èƒ½ãªãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        races = []

        # CSV ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡º
        for filename, df in self.csv_data.items():
            if not df.empty:
                race_info = {
                    'source': 'CSV',
                    'filename': filename,
                    'rows': len(df),
                    'columns': len(df.columns),
                    'date_range': self._get_date_range(df)
                }
                races.append(race_info)

        self.available_races = races
        logger.info(f"åˆ©ç”¨å¯èƒ½ãªãƒ¬ãƒ¼ã‚¹æ•°: {len(races)}")
        return races

    def _get_date_range(self, df):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ—¥ä»˜ç¯„å›²ã‚’å–å¾—"""
        date_cols = []
        for col in df.columns:
            if df[col].dtype == 'datetime64[ns]':
                date_cols.append(col)

        if date_cols:
            try:
                min_date = df[date_cols[0]].min()
                max_date = df[date_cols[0]].max()
                return f"{min_date.strftime('%Y-%m-%d')} ~ {max_date.strftime('%Y-%m-%d')}"
            except:
                return "æ—¥ä»˜ç¯„å›²ä¸æ˜"
        return "æ—¥ä»˜ãªã—"

    def _display_debug_info(self):
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º"""
        logger.info("=== ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===")
        logger.info(f"æ¤œç´¢ã•ã‚ŒãŸãƒ‘ã‚¹æ•°: {len(self.debug_info['searched_paths'])}")

        for path in self.debug_info['searched_paths']:
            logger.info(f"  æ¤œç´¢ãƒ‘ã‚¹: {path}")

        logger.info(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(self.debug_info['found_files'])}")
        for path, files in self.debug_info['found_files'].items():
            logger.info(f"  {path}: {len(files)} ãƒ•ã‚¡ã‚¤ãƒ«")

        if self.debug_info['errors']:
            logger.info(f"ã‚¨ãƒ©ãƒ¼æ•°: {len(self.debug_info['errors'])}")
            for error in self.debug_info['errors']:
                logger.error(f"  {error}")

    def get_debug_summary(self):
        """ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ã‚’è¿”ã™"""
        return {
            'csv_data_path': str(self.csv_data_path) if self.csv_data_path else "æœªç™ºè¦‹",
            'db_path': str(self.db_path) if self.db_path else "æœªç™ºè¦‹",
            'csv_files_loaded': len(self.csv_data),
            'total_rows': sum(len(df) for df in self.csv_data.values()),
            'available_races': len(self.available_races),
            'errors': len(self.debug_info['errors']),
            'searched_paths': len(self.debug_info['searched_paths'])
        }

class KyoteiAIPrediction:
    """ç«¶è‰‡AIäºˆæƒ³ã‚¯ãƒ©ã‚¹ - å®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç‰ˆ"""

    def __init__(self, data_manager):
        self.data_manager = data_manager

    def analyze_racer_performance(self, racer_data):
        """é¸æ‰‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        if racer_data.empty:
            return {}

        try:
            analysis = {
                'total_races': len(racer_data),
                'win_rate': (racer_data.get('ç€é †', pd.Series()).eq(1).sum() / len(racer_data) * 100) if 'ç€é †' in racer_data.columns else 0,
                'avg_start_timing': racer_data.get('ST', pd.Series()).mean() if 'ST' in racer_data.columns else 0,
                'recent_performance': self._get_recent_performance(racer_data)
            }
            return analysis
        except Exception as e:
            logger.error(f"é¸æ‰‹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _get_recent_performance(self, racer_data, days=30):
        """ç›´è¿‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        if racer_data.empty:
            return "ãƒ‡ãƒ¼ã‚¿ãªã—"

        # æ—¥ä»˜åˆ—ã‚’æ¢ã™
        date_col = None
        for col in racer_data.columns:
            if 'date' in col.lower() or 'æ—¥ä»˜' in col:
                date_col = col
                break

        if date_col is None:
            return "æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãªã—"

        try:
            recent_date = datetime.now() - timedelta(days=days)
            recent_data = racer_data[pd.to_datetime(racer_data[date_col]) >= recent_date]

            if len(recent_data) == 0:
                return "ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ãªã—"

            wins = recent_data.get('ç€é †', pd.Series()).eq(1).sum()
            return f"ç›´è¿‘{days}æ—¥: {len(recent_data)}æˆ¦{wins}å‹"
        except Exception as e:
            logger.error(f"ç›´è¿‘ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return "åˆ†æã‚¨ãƒ©ãƒ¼"

    def predict_race_outcome(self, race_data):
        """ãƒ¬ãƒ¼ã‚¹çµæœäºˆæƒ³"""
        if race_data.empty:
            return {}

        try:
            # åŸºæœ¬çš„ãªäºˆæƒ³ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
            predictions = {}

            if 'ã‚ªãƒƒã‚º' in race_data.columns:
                # ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ã®åˆ†æ
                odds_analysis = self._analyze_odds(race_data)
                predictions.update(odds_analysis)

            if 'ST' in race_data.columns:
                # ã‚¹ã‚¿ãƒ¼ãƒˆåˆ†æ
                start_analysis = self._analyze_start_performance(race_data)
                predictions.update(start_analysis)

            return predictions

        except Exception as e:
            logger.error(f"ãƒ¬ãƒ¼ã‚¹äºˆæƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _analyze_odds(self, race_data):
        """ã‚ªãƒƒã‚ºåˆ†æ"""
        try:
            odds_col = 'ã‚ªãƒƒã‚º'
            if odds_col in race_data.columns:
                min_odds = race_data[odds_col].min()
                favorite = race_data[race_data[odds_col] == min_odds].iloc[0]

                return {
                    'favorite_boat': favorite.get('è‰‡ç•ª', 'N/A'),
                    'favorite_odds': min_odds,
                    'odds_analysis': "ã‚ªãƒƒã‚ºåˆ†æå®Œäº†"
                }
        except Exception as e:
            logger.error(f"ã‚ªãƒƒã‚ºåˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        return {}

    def _analyze_start_performance(self, race_data):
        """ã‚¹ã‚¿ãƒ¼ãƒˆåˆ†æ"""
        try:
            st_col = 'ST'
            if st_col in race_data.columns:
                best_st = race_data[st_col].min()
                best_starter = race_data[race_data[st_col] == best_st].iloc[0]

                return {
                    'best_starter': best_starter.get('è‰‡ç•ª', 'N/A'),
                    'best_st': best_st,
                    'start_analysis': "ã‚¹ã‚¿ãƒ¼ãƒˆåˆ†æå®Œäº†"
                }
        except Exception as e:
            logger.error(f"ã‚¹ã‚¿ãƒ¼ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        return {}

    def generate_prediction_report(self, selected_data):
        """äºˆæƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not selected_data or len(selected_data) == 0:
            return "åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"

        try:
            report = []
            report.append("# ç«¶è‰‡AIäºˆæƒ³ãƒ¬ãƒãƒ¼ãƒˆ")
            report.append(f"## ãƒ‡ãƒ¼ã‚¿æ¦‚è¦")
            report.append(f"- åˆ†æå¯¾è±¡: {len(selected_data)} ãƒ¬ãƒ¼ã‚¹")
            report.append(f"- ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            # ãƒ‡ãƒ¼ã‚¿åˆ¥åˆ†æ
            for filename, df in selected_data.items():
                report.append(f"\n### {filename}")
                report.append(f"- ãƒ¬ãƒ¼ã‚¹æ•°: {len(df)}")

                if not df.empty:
                    # åŸºæœ¬çµ±è¨ˆ
                    if 'ç€é †' in df.columns:
                        win_rate = (df['ç€é †'].eq(1).sum() / len(df) * 100)
                        report.append(f"- 1ç€ç‡: {win_rate:.1f}%")

                    if 'ã‚ªãƒƒã‚º' in df.columns:
                        avg_odds = df['ã‚ªãƒƒã‚º'].mean()
                        report.append(f"- å¹³å‡ã‚ªãƒƒã‚º: {avg_odds:.2f}")

            report.append("\n## AIäºˆæƒ³ã®ãƒã‚¤ãƒ³ãƒˆ")
            report.append("1. **å®Ÿãƒ‡ãƒ¼ã‚¿åˆ†æ**: éå»ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«åˆ†æ")
            report.append("2. **å¤šè§’çš„è©•ä¾¡**: ã‚ªãƒƒã‚ºã€ã‚¹ã‚¿ãƒ¼ãƒˆã€é¸æ‰‹æˆç¸¾ã‚’ç·åˆè©•ä¾¡") 
            report.append("3. **ãƒªã‚¹ã‚¯ç®¡ç†**: ç¢ºç‡è«–ã«åŸºã¥ãå …å®Ÿãªäºˆæƒ³")

            return "\n".join(report)

        except Exception as e:
            logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

def create_note_article(prediction_report, race_data_summary):
    """noteè¨˜äº‹ç”Ÿæˆ"""
    try:
        article = []
        article.append("# ğŸ ç«¶è‰‡AIäºˆæƒ³ - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã§å‹ç‡UP!")
        article.append("")
        article.append("ã“ã‚“ã«ã¡ã¯ï¼ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã®ç§ãŒã€")
        article.append("æœ€æ–°ã®ç«¶è‰‡ãƒ‡ãƒ¼ã‚¿ã‚’å¾¹åº•åˆ†æã—ã¦ãŠé€ã‚Šã™ã‚‹äºˆæƒ³è¨˜äº‹ã§ã™ã€‚")
        article.append("")

        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
        article.append("## ğŸ“Š ä»Šå›ã®åˆ†æãƒ‡ãƒ¼ã‚¿")
        article.append(f"- å¯¾è±¡ãƒ¬ãƒ¼ã‚¹æ•°: {race_data_summary.get('total_races', 0)}")
        article.append(f"- åˆ†ææœŸé–“: {race_data_summary.get('period', 'ç›´è¿‘ãƒ‡ãƒ¼ã‚¿')}")
        article.append("")

        # äºˆæƒ³ãƒ¬ãƒãƒ¼ãƒˆæŒ¿å…¥
        article.append("## ğŸ¤– AIåˆ†æçµæœ")
        article.append(prediction_report)
        article.append("")

        # noteè¨˜äº‹ç”¨ã®ã¾ã¨ã‚
        article.append("## ğŸ’¡ ä»Šæ—¥ã®ãƒã‚¤ãƒ³ãƒˆ")
        article.append("1. **ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³**: æ„Ÿè¦šã§ã¯ãªãã€ãƒ‡ãƒ¼ã‚¿ã§åˆ¤æ–­")
        article.append("2. **ç¢ºç‡è«–æ€è€ƒ**: 100%ã¯ãªã„ã€ç¢ºç‡ã§è€ƒãˆã‚‹")
        article.append("3. **ç¶™ç¶šæ”¹å–„**: çµæœã‚’æ¤œè¨¼ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„")
        article.append("")

        article.append("## ğŸ¯ å…è²¬äº‹é …")
        article.append("ã“ã®äºˆæƒ³ã¯éå»ãƒ‡ãƒ¼ã‚¿ã®åˆ†æçµæœã§ã™ã€‚")
        article.append("æŠ•è³‡ã¯è‡ªå·±è²¬ä»»ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚")
        article.append("")

        article.append("---")
        article.append("ãƒ‡ãƒ¼ã‚¿ã§å‹ã¤ç«¶è‰‡äºˆæƒ³ã€ã„ã‹ãŒã§ã—ãŸã‹ï¼Ÿ")
        article.append("ãƒ•ã‚©ãƒ­ãƒ¼ãƒ»ã‚¹ã‚­ã§å¿œæ´ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ï¼")

        return "\n".join(article)

    except Exception as e:
        logger.error(f"noteè¨˜äº‹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return f"noteè¨˜äº‹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    st.set_page_config(
        page_title="ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  v13.9 (Debug)",
        page_icon="ğŸ",
        layout="wide"
    )

    st.title("ğŸ ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  v13.9 (Debugç‰ˆ)")
    st.caption("ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è‡ªå‹•æ¤œç´¢ãƒ»ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½å¼·åŒ–ç‰ˆ")

    # ãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    if 'data_manager' not in st.session_state:
        st.session_state.data_manager = KyoteiDataManager()

    data_manager = st.session_state.data_manager

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
    with st.expander("ğŸ” ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
        debug_summary = data_manager.get_debug_summary()

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("ãƒ‡ãƒ¼ã‚¿ç™ºè¦‹çŠ¶æ³")
            st.info(f"ğŸ“ CSVãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: `{debug_summary['csv_data_path']}`")
            st.info(f"ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹: `{debug_summary['db_path']}`")
            st.success(f"âœ… èª­ã¿è¾¼ã¿æ¸ˆã¿CSV: {debug_summary['csv_files_loaded']} ãƒ•ã‚¡ã‚¤ãƒ«")
            st.success(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿è¡Œæ•°: {debug_summary['total_rows']:,} è¡Œ")

        with col2:
            st.subheader("æ¤œç´¢ãƒ»ã‚¨ãƒ©ãƒ¼çŠ¶æ³")
            st.info(f"ğŸ” æ¤œç´¢ãƒ‘ã‚¹æ•°: {debug_summary['searched_paths']}")
            st.info(f"ğŸ åˆ©ç”¨å¯èƒ½ãƒ¬ãƒ¼ã‚¹: {debug_summary['available_races']}")

            if debug_summary['errors'] > 0:
                st.error(f"âš ï¸ ã‚¨ãƒ©ãƒ¼æ•°: {debug_summary['errors']}")
            else:
                st.success("âœ… ã‚¨ãƒ©ãƒ¼ãªã—")

        # è©³ç´°ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        if st.button("ğŸ“‹ è©³ç´°ãƒ­ã‚°ã‚’è¡¨ç¤º"):
            st.subheader("æ¤œç´¢ãƒ‘ã‚¹ä¸€è¦§")
            for path in data_manager.debug_info['searched_paths']:
                st.text(f"â€¢ {path}")

            if data_manager.debug_info['found_files']:
                st.subheader("ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«")
                for path, files in data_manager.debug_info['found_files'].items():
                    st.text(f"ğŸ“ {path}")
                    for file in files:
                        st.text(f"  â€¢ {file}")

            if data_manager.debug_info['errors']:
                st.subheader("ã‚¨ãƒ©ãƒ¼è©³ç´°")
                for error in data_manager.debug_info['errors']:
                    st.error(error)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if st.button("ğŸ”„ ãƒ‡ãƒ¼ã‚¿å†èª­ã¿è¾¼ã¿"):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            data_manager.load_csv_data()
            data_manager.connect_database()
            st.success("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†ï¼")
            st.rerun()

    # åˆæœŸãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    if not data_manager.csv_data:
        st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸Šã®ã€Œãƒ‡ãƒ¼ã‚¿å†èª­ã¿è¾¼ã¿ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # ãƒ¡ã‚¤ãƒ³æ©Ÿèƒ½
    st.header("ğŸ¯ ç«¶è‰‡AIäºˆæƒ³")

    # ãƒ‡ãƒ¼ã‚¿é¸æŠ
    available_data = list(data_manager.csv_data.keys())
    selected_files = st.multiselect(
        "ğŸ“Š åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠ",
        available_data,
        default=available_data[:3] if len(available_data) >= 3 else available_data
    )

    if not selected_files:
        st.warning("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠã—ã¦ãã ã•ã„")
        st.stop()

    # é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦è¡¨ç¤º
    col1, col2, col3 = st.columns(3)

    selected_data = {f: data_manager.csv_data[f] for f in selected_files}
    total_rows = sum(len(df) for df in selected_data.values())

    with col1:
        st.metric("é¸æŠãƒ•ã‚¡ã‚¤ãƒ«æ•°", len(selected_files))
    with col2:
        st.metric("ç·ãƒ¬ãƒ¼ã‚¹æ•°", f"{total_rows:,}")
    with col3:
        avg_rows = total_rows // len(selected_files) if selected_files else 0
        st.metric("ãƒ•ã‚¡ã‚¤ãƒ«å¹³å‡è¡Œæ•°", f"{avg_rows:,}")

    # AIäºˆæƒ³å®Ÿè¡Œ
    if st.button("ğŸ¤– AIäºˆæƒ³ã‚’å®Ÿè¡Œ", type="primary"):
        with st.spinner("AIåˆ†æä¸­..."):
            # AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            ai_predictor = KyoteiAIPrediction(data_manager)

            # äºˆæƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            prediction_report = ai_predictor.generate_prediction_report(selected_data)

            # ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
            race_data_summary = {
                'total_races': total_rows,
                'period': 'éå»ãƒ‡ãƒ¼ã‚¿',
                'files': len(selected_files)
            }

            st.success("âœ… AIåˆ†æå®Œäº†ï¼")

            # çµæœè¡¨ç¤º
            tab1, tab2, tab3 = st.tabs(["ğŸ“Š äºˆæƒ³çµæœ", "ğŸ“ noteè¨˜äº‹", "ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿è©³ç´°"])

            with tab1:
                st.subheader("ğŸ¤– AIäºˆæƒ³ãƒ¬ãƒãƒ¼ãƒˆ")
                st.markdown(prediction_report)

            with tab2:
                st.subheader("ğŸ“ noteæŠ•ç¨¿ç”¨è¨˜äº‹")
                note_article = create_note_article(prediction_report, race_data_summary)
                st.markdown(note_article)

                # ã‚³ãƒ”ãƒ¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢
                st.text_area("ğŸ“‹ ã‚³ãƒ”ãƒ¼ç”¨ãƒ†ã‚­ã‚¹ãƒˆ", note_article, height=300)

            with tab3:
                st.subheader("ğŸ“ˆ ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿è©³ç´°")

                for filename, df in selected_data.items():
                    with st.expander(f"ğŸ“Š {filename} (è¡Œæ•°: {len(df):,})"):
                        st.dataframe(df.head())

                        # åŸºæœ¬çµ±è¨ˆ
                        if not df.empty:
                            st.subheader("åŸºæœ¬çµ±è¨ˆ")
                            numeric_cols = df.select_dtypes(include=[np.number]).columns
                            if len(numeric_cols) > 0:
                                st.dataframe(df[numeric_cols].describe())

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.caption("ğŸ ç«¶è‰‡AIäºˆæƒ³ã‚·ã‚¹ãƒ†ãƒ  v13.9 - Debugç‰ˆ | ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹è‡ªå‹•æ¤œç´¢ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–")
    st.caption(f"ğŸ’¾ ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿: CSV {len(data_manager.csv_data)} ãƒ•ã‚¡ã‚¤ãƒ«, ç·è¡Œæ•° {sum(len(df) for df in data_manager.csv_data.values()):,}")

if __name__ == "__main__":
    main()
