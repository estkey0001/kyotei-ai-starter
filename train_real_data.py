import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import joblib
import warnings
warnings.filterwarnings('ignore')

print("ğŸš€ æˆ¸ç”°ç«¶è‰‡2024å¹´å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
df = pd.read_csv('data/coconala_2024/toda_2024.csv')
print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
print(f"ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['race_date'].min()} - {df['race_date'].max()}")

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
print("ğŸ”§ å‰å‡¦ç†å®Ÿè¡Œä¸­...")
# äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: 1ç€äºˆæ¸¬
target_cols = ['finish_position_1', 'finish_position_2', 'finish_position_3', 
               'finish_position_4', 'finish_position_5', 'finish_position_6']

# å„ãƒ¬ãƒ¼ã‚¹ã§1ç€ã‚’ç‰¹å®š
y = []
features = []

for idx, row in df.iterrows():
    positions = [row[col] for col in target_cols if pd.notna(row[col])]
    if 1.0 in positions:
        winner = positions.index(1.0)  # 0-5ã®è‰‡ç•ª
        y.append(winner)
        
        # ç‰¹å¾´é‡é¸æŠ
        feature_row = []
        
        # æ°—è±¡ãƒ‡ãƒ¼ã‚¿
        feature_row.extend([
            row.get('temperature', 0),
            row.get('wind_speed', 0),
            row.get('wave_height', 0)
        ])
        
        # å„è‰‡ã®ç‰¹å¾´é‡
        for i in range(1, 7):
            feature_row.extend([
                row.get(f'win_rate_national_{i}', 0),
                row.get(f'place_rate_2_national_{i}', 0),
                row.get(f'racer_age_{i}', 0),
                row.get(f'motor_win_rate_{i}', 0),
                row.get(f'avg_start_timing_{i}', 0)
            ])
        
        features.append(feature_row)

X = np.array(features)
y = np.array(y)

print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(X)}ãƒ¬ãƒ¼ã‚¹, {X.shape[1]}ç‰¹å¾´é‡")
print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y)}")

# å­¦ç¿’å®Ÿè¡Œ
print("ğŸ¤– æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

best_model = None
best_score = 0

for name, model in models.items():
    # äº¤å·®æ¤œè¨¼
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    mean_score = cv_scores.mean()
    
    print(f"{name}: CVç²¾åº¦ = {mean_score:.4f} (Â±{cv_scores.std()*2:.4f})")
    
    if mean_score > best_score:
        best_score = mean_score
        best_model = model

# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’
print(f"ğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠ: {best_model.__class__.__name__}")
best_model.fit(X_train, y_train)

# ãƒ†ã‚¹ãƒˆç²¾åº¦è©•ä¾¡
test_pred = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_pred)

print(f"âœ… ãƒ†ã‚¹ãƒˆç²¾åº¦: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)")
print(f"ğŸ¯ ç²¾åº¦å‘ä¸Š: {((test_accuracy - 0.823) / 0.823 * 100):+.1f}%")

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜
model_path = 'toda_2024_high_accuracy_model.pkl'
joblib.dump(best_model, model_path)
print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

print("ğŸ‰ å­¦ç¿’å®Œäº†ï¼")
